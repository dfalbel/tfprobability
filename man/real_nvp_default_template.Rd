% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bijectors.R
\name{real_nvp_default_template}
\alias{real_nvp_default_template}
\title{Build a scale-and-shift function using a multi-layer neural network.}
\usage{
real_nvp_default_template(hidden_layers, shift_only = FALSE,
  activation = tf$nn$relu, name = NULL, ...)
}
\arguments{
\item{hidden_layers}{list-like of non-negative integer, scalars indicating the number
of units in each hidden layer. Default: list(512, 512).}

\item{shift_only}{logical indicating if only the shift term shall be
computed (i.e. NICE bijector). Default: FALSE.}

\item{activation}{Activation function (callable). Explicitly setting to NULL implies a linear activation.}

\item{name}{A name for ops managed by this function. Default: "real_nvp_default_template".}

\item{...}{tf$layers$dense arguments}
}
\description{
This will be wrapped in a make_template to ensure the variables are only
created once. It takes the d-dimensional input x[0:d] and returns the D-d
dimensional outputs loc ("mu") and log_scale ("alpha").
}
\details{
The default template does not support conditioning and will raise an
exception if condition_kwargs are passed to it. To use conditioning in
real nvp bijector, implement a conditioned shift/scale template that
handles the condition_kwargs.

References
[1]: George Papamakarios, Theo Pavlakou, and Iain Murray.
Masked Autoregressive Flow for Density Estimation.
In _Neural Information Processing Systems_, 2017. https://arxiv.org/abs/1705.07057
}
\seealso{
Other bijectors: \code{\link{masked_autoregressive_default_template}},
  \code{\link{masked_dense}},
  \code{\link{tfb_absolute_value}},
  \code{\link{tfb_affine_linear_operator}},
  \code{\link{tfb_affine_scalar}},
  \code{\link{tfb_affine}},
  \code{\link{tfb_batch_normalization}},
  \code{\link{tfb_blockwise}}, \code{\link{tfb_chain}},
  \code{\link{tfb_cholesky_outer_product}},
  \code{\link{tfb_cholesky_to_inv_cholesky}},
  \code{\link{tfb_discrete_cosine_transform}},
  \code{\link{tfb_expm1}}, \code{\link{tfb_exp}},
  \code{\link{tfb_fill_triangular}},
  \code{\link{tfb_gumbel}}, \code{\link{tfb_identity}},
  \code{\link{tfb_inline}}, \code{\link{tfb_invert}},
  \code{\link{tfb_kumaraswamy}},
  \code{\link{tfb_masked_autoregressive_flow}},
  \code{\link{tfb_matrix_inverse_tri_l}},
  \code{\link{tfb_matvec_lu}},
  \code{\link{tfb_normal_cdf}}, \code{\link{tfb_ordered}},
  \code{\link{tfb_permute}},
  \code{\link{tfb_power_transform}},
  \code{\link{tfb_real_nvp}}, \code{\link{tfb_reciprocal}},
  \code{\link{tfb_reshape}}, \code{\link{tfb_scale_tri_l}},
  \code{\link{tfb_sigmoid}},
  \code{\link{tfb_sinh_arcsinh}},
  \code{\link{tfb_softmax_centered}},
  \code{\link{tfb_softplus}}, \code{\link{tfb_softsign}},
  \code{\link{tfb_square}}, \code{\link{tfb_tanh}},
  \code{\link{tfb_transform_diagonal}},
  \code{\link{tfb_transpose}}, \code{\link{tfb_weibull}}
}
\concept{bijectors}
